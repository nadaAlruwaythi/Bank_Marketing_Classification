{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eec306d",
   "metadata": {},
   "source": [
    "# Bank-Marketing-Classification\n",
    "___\n",
    ">In this project our task was to get a dataset to build a classification model on. We choose to analysis a data set of a bank to determine if the customers will subscribe to a service the bank is offering. Our target will be (yes or no), as a result, the type of the model we will build is a binary classification model. The dataset will be obtained from the uci.edu website and will consist of 45212 rows and 17 columns in total.\n",
    "> - https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "\n",
    "# The Approach:\n",
    "___\n",
    "> - analyzing the dataset using EDA techniques.\n",
    "> - Building a classification model to predict the people who are most likely to subscribe to the service the bank is offering.\n",
    "\n",
    "# Design & data preparation:\n",
    "___\n",
    "> **1- in this dataset we had:**\n",
    "> - 9 categorical features\n",
    "> - feature job and month has the highest number of categorical values\n",
    "\n",
    "> **2- data preparation:**\n",
    "> - We turned categorical features into dummies values \n",
    "> - There was no nulls or duplicates rows\n",
    "> - Our target data was not balanced so we balance it by using RandomOverSampler\n",
    "> - We used 6 kinds of models to find the best score for our classification\n",
    "\n",
    "# Features engineering:\n",
    "___\n",
    "> - We got the Features importance for our chosen model and raised the final score of the training and validation sets to 82% and 83% respectively. \n",
    "> - We found out that there is a high correlation between the \"poutcome_success\" column and our target column \"y\" so we multiplied those columns together to get a better score, but that didnâ€™t affect the score that much. \n",
    "\n",
    "# Classification:\n",
    "___\n",
    "> Here we used 6 types of classification models, and the table below compares between them according to :\n",
    "> Accuracy, F1, Precision, Recall, and ROC Score.\n",
    "\n",
    "| Model                 | Accuracy | F1    | Precision  | Recall  | ROC  |\n",
    "|:----                  |:-------: |:-----:|:----------:|:-------:|----: |\n",
    "| LogisticRegression    | 0.8658   |0.5247 |0.4580\t    |0.6141\t  |0.7572|\n",
    "| KNeighborsClassifier  | 0.7763   |0.3531 |0.2711\t    |0.5060\t  |0.6597|\n",
    "| RandomForestClassifier|0.8983    |0.5267 |0.6002\t    |0.4693\t  |0.7132|\n",
    "| DecisionTreeClassifier|0.8674    |0.4395 |0.4485\t    |0.4308\t  |0.6791|\n",
    "| XGBClassifier         |0.8853    |0.5909 |0.5187\t    |0.6865\t  |0.7996|\n",
    "| BaggingClassifier     |0.8944    |0.5497 |0.5660\t    |0.5344\t  |0.7391|\n",
    "\n",
    "# Result & notes:\n",
    "___\n",
    "> - The model which gave us the highest scores in both the training and validation sets are the Logistic Regression model with a score of 82% and 83% respectively.\n",
    "> - In the test set the Logistic Regression gave us 86%, which is even better than the scores of the training and the validation sets. \n",
    "> - last, we were able to Deploy our model to the website flask and predict the results according to the chosen features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0be0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
